{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Basic_CNN_for_cancer_detection [PyTorch].ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamilo116/AI/blob/master/Basic_CNN_for_cancer_detection_%5BPyTorch%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "scrolled": true,
        "id": "QxXwI_liNjGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "import sys\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import timeit\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlyqUAkoSYFJ",
        "colab_type": "code",
        "outputId": "89044cb8-d3cf-4c3b-cc88-bed822af961f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNOTAtoMYAr8",
        "colab_type": "code",
        "outputId": "dd9cb0a0-cd68-4bbd-e4d2-7b02f0526ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "!pip3 install kaggle\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.6.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.11.28)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Moqq4VoWo5i5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file_list = os.listdir('/content/drive/My Drive/Colab_data/train/')\n",
        "# file_ids = []\n",
        "# for f in file_list:\n",
        "#     file_ids.append(f.split('.')[0])\n",
        "# print(len(file_ids))\n",
        "\n",
        "# print(file_list)\n",
        "# labels = pd.read_csv('/content/drive/My Drive/Colab_data/train_labels.csv')\n",
        "# print(len(labels))\n",
        "# new_id_list = []\n",
        "# for idi, row in labels.iterrows():\n",
        "#     #print(row['id'])\n",
        "#     if row['id'] not in file_ids:\n",
        "#       labels.drop(idi, inplace=True)\n",
        "      \n",
        "#       #file_list.remove(row['id'] + '.tif')\n",
        "#     else:\n",
        "#       print(\"id \" + str(idi) + \" in filelist\" )\n",
        "\n",
        "\n",
        "\n",
        "# print(len(labels))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9K_ksc-ZFv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO3JBuppr6DG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(labels)\n",
        "#labels.to_csv('selected_train.csv', sep = '\\t', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeJJEaqgJbMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with open('/content/drive/My Drive/Colab_data/selected_train.csv') as f:\n",
        "#   for line in f.readlines():\n",
        "#     line.replace('\\t', ',')\n",
        "#     print(line)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sRoz4cMsnyg",
        "colab_type": "code",
        "outputId": "3b8b5fcc-8336-4cb0-d5c0-da9ba6161e63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "! cp 'selected_train.csv' '/content/drive/My Drive/Colab_data/selected_train.csv'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'selected_train.csv': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwGagmrdaUje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! cp /content/drive/My\\ Drive/Colab_data/kaggle.json /root/.kaggle/kaggle.json\n",
        "#! chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMnjTv8CZ_3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! kaggle datasets download --path \"/content/drive/My\\ Drive/Colab_data\" histopathologic-cancer-detection\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyf41Tk9S0-B",
        "colab_type": "code",
        "outputId": "2a022129-b779-4899-911a-5e0b61ea5265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        " ! git clone https://github.com/wang-chen/kervolution.git \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'kervolution'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects:  16% (1/6)\u001b[K\rremote: Counting objects:  33% (2/6)\u001b[K\rremote: Counting objects:  50% (3/6)\u001b[K\rremote: Counting objects:  66% (4/6)\u001b[K\rremote: Counting objects:  83% (5/6)\u001b[K\rremote: Counting objects: 100% (6/6)\u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects:  16% (1/6)\u001b[K\rremote: Compressing objects:  33% (2/6)\u001b[K\rremote: Compressing objects:  50% (3/6)\u001b[K\rremote: Compressing objects:  66% (4/6)\u001b[K\rremote: Compressing objects:  83% (5/6)\u001b[K\rremote: Compressing objects: 100% (6/6)\u001b[K\rremote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "Unpacking objects:   1% (1/53)   \rUnpacking objects:   3% (2/53)   \rUnpacking objects:   5% (3/53)   \rUnpacking objects:   7% (4/53)   \rUnpacking objects:   9% (5/53)   \rUnpacking objects:  11% (6/53)   \rUnpacking objects:  13% (7/53)   \rUnpacking objects:  15% (8/53)   \rUnpacking objects:  16% (9/53)   \rUnpacking objects:  18% (10/53)   \rUnpacking objects:  20% (11/53)   \rUnpacking objects:  22% (12/53)   \rUnpacking objects:  24% (13/53)   \rUnpacking objects:  26% (14/53)   \rUnpacking objects:  28% (15/53)   \rUnpacking objects:  30% (16/53)   \rUnpacking objects:  32% (17/53)   \rUnpacking objects:  33% (18/53)   \rremote: Total 53 (delta 2), reused 0 (delta 0), pack-reused 47\u001b[K\n",
            "Unpacking objects:  35% (19/53)   \rUnpacking objects:  37% (20/53)   \rUnpacking objects:  39% (21/53)   \rUnpacking objects:  41% (22/53)   \rUnpacking objects:  43% (23/53)   \rUnpacking objects:  45% (24/53)   \rUnpacking objects:  47% (25/53)   \rUnpacking objects:  49% (26/53)   \rUnpacking objects:  50% (27/53)   \rUnpacking objects:  52% (28/53)   \rUnpacking objects:  54% (29/53)   \rUnpacking objects:  56% (30/53)   \rUnpacking objects:  58% (31/53)   \rUnpacking objects:  60% (32/53)   \rUnpacking objects:  62% (33/53)   \rUnpacking objects:  64% (34/53)   \rUnpacking objects:  66% (35/53)   \rUnpacking objects:  67% (36/53)   \rUnpacking objects:  69% (37/53)   \rUnpacking objects:  71% (38/53)   \rUnpacking objects:  73% (39/53)   \rUnpacking objects:  75% (40/53)   \rUnpacking objects:  77% (41/53)   \rUnpacking objects:  79% (42/53)   \rUnpacking objects:  81% (43/53)   \rUnpacking objects:  83% (44/53)   \rUnpacking objects:  84% (45/53)   \rUnpacking objects:  86% (46/53)   \rUnpacking objects:  88% (47/53)   \rUnpacking objects:  90% (48/53)   \rUnpacking objects:  92% (49/53)   \rUnpacking objects:  94% (50/53)   \rUnpacking objects:  96% (51/53)   \rUnpacking objects:  98% (52/53)   \rUnpacking objects: 100% (53/53)   \rUnpacking objects: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d18642c1bcaec05236c28de039e7ce2f6512f6d3",
        "id": "67CzNDCdNjHG",
        "colab_type": "text"
      },
      "source": [
        "so we have 4 files to work with.\n",
        "and we will load in some librarys, there will probably mix of pytourch, sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UluKqEnDNjHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sys.path.append(\"kervolution/\")\n",
        "from kervolution import Kerv2d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY2O_P-rNjH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_OlBHw6NjIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class LinearKernel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearKernel, self).__init__()\n",
        "    \n",
        "    def forward(self, x_unf, w, b):\n",
        "        t = x_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2)\n",
        "        if b is not None:\n",
        "            return t + b\n",
        "        return t\n",
        "        \n",
        "        \n",
        "class PolynomialKernel(LinearKernel):\n",
        "    def __init__(self, cp=2.0, dp=3, train_cp=True):\n",
        "        super(PolynomialKernel, self).__init__()\n",
        "        self.cp = torch.nn.parameter.Parameter(torch.tensor(cp, requires_grad=train_cp))\n",
        "        self.dp = dp\n",
        "\n",
        "    def forward(self, x_unf, w, b):\n",
        "        return (self.cp + super(PolynomialKernel, self).forward(x_unf, w, b))**self.dp\n",
        "\n",
        "\n",
        "class GaussianKernel(torch.nn.Module):\n",
        "    def __init__(self, gamma):\n",
        "        super(GaussianKernel, self).__init__()\n",
        "        self.gamma = torch.nn.parameter.Parameter(\n",
        "                            torch.tensor(gamma, requires_grad=True))\n",
        "    \n",
        "    def forward(self, x_unf, w, b):\n",
        "        l = x_unf.transpose(1, 2)[:, :, :, None] - w.view(1, 1, -1, w.size(0))\n",
        "        l = torch.sum(l**2, 2)\n",
        "        t = torch.exp(-self.gamma * l)\n",
        "        if b:\n",
        "            return t + b\n",
        "        return t\n",
        "        \n",
        "       \n",
        "class KernelConv2d(torch.nn.Conv2d):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, kernel_fn=PolynomialKernel,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, bias=None,\n",
        "                 padding_mode='zeros'):\n",
        "        '''\n",
        "        Follows the same API as torch Conv2d except kernel_fn.\n",
        "        kernel_fn should be an instance of the above kernels.\n",
        "        '''\n",
        "        super(KernelConv2d, self).__init__(in_channels, out_channels, \n",
        "                                           kernel_size, stride, padding,\n",
        "                                           dilation, groups, bias, padding_mode)\n",
        "        self.kernel_fn = kernel_fn()\n",
        "   \n",
        "    def compute_shape(self, x):\n",
        "        h = (x.shape[2] + 2 * self.padding[0] - 1 * (self.kernel_size[0] - 1) - 1) // self.stride[0] + 1\n",
        "        w = (x.shape[3] + 2 * self.padding[1] - 1 * (self.kernel_size[1] - 1) - 1) // self.stride[1] + 1\n",
        "        return h, w\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x_unf = torch.nn.functional.unfold(x, self.kernel_size, self.dilation,self.padding, self.stride)\n",
        "        h, w = self.compute_shape(x)\n",
        "        return self.kernel_fn(x_unf, self.weight, self.bias).view(x.shape[0], -1, h, w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "wRJRDtndNjIA",
        "colab_type": "code",
        "outputId": "ea2c070d-a08c-44f3-bd84-ce00f8503e2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "class CancerDataset(Dataset):\n",
        "    def __init__(self, datafolder, datatype='train', transform = transforms.Compose([transforms.ToTensor()]), labels_dict={}):\n",
        "        \n",
        "        os.listdir(datafolder)\n",
        "        self.datafolder = datafolder\n",
        "        self.datatype = datatype\n",
        "        self.image_files_list = [s for s in os.listdir(datafolder)]\n",
        "        self.transform = transform\n",
        "        self.labels_dict = labels_dict\n",
        "        if self.datatype == 'train':\n",
        "            lab = [labels_dict[i.split('.')[0]] for i in self.image_files_list]\n",
        "            self.labels = lab\n",
        "            print(lab)\n",
        "        else:\n",
        "            self.labels = [0 for _ in range(len(self.image_files_list))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #print(\"idx \" + str(idx))\n",
        "        #print(\"self.datafolder \" +  self.datafolder)\n",
        "        #print(\"self.image_files_list[idx] \" + self.image_files_list[idx])\n",
        "        img_name = os.path.join(self.datafolder, self.image_files_list[idx])\n",
        "        image = Image.open(img_name)\n",
        "        image = self.transform(image)\n",
        "        img_name_short = self.image_files_list[idx].split('.')[0]\n",
        "\n",
        "        if self.datatype == 'train':\n",
        "            label = self.labels_dict[img_name_short]\n",
        "        else:\n",
        "            label = 0\n",
        "        return image, label\n",
        "\n",
        "\n",
        "IMAGE_NOT_FOUND_COUNTER = 0\n",
        "\n",
        "labels = pd.read_csv('/content/drive/My Drive/Colab_data/selected_train.csv', sep='\\t') #, header=None, usecols=[0,1])\n",
        "#print(labels)\n",
        "file_list = os.listdir('/content/drive/My Drive/Colab_data/train/')\n",
        "#print(file_list)\n",
        "#new_labels = pd.DataFrame(columns=('id', 'label'))\n",
        "\n",
        "#for idi, label in labels.iteritems():\n",
        "#    for f in file_list:\n",
        "#        #f = f.split('.')[0]\n",
        "#        if str(idi) in f:\n",
        "#            new_labels.append({'id': idi, 'label': label}, ignore_index=True)\n",
        "     \n",
        "#print(new_labels)\n",
        "#pd.DataFrame(my_list, columns = list(\"abc\"))\n",
        "#labels = pd.Dataframe(new_labels, columns = )\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "    #transforms.CenterCrop(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "data_transforms_test = transforms.Compose([\n",
        "    #transforms.CenterCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "tr, val = train_test_split(labels.label, stratify=labels.label, test_size=0.1)\n",
        "print(\"number of training data: \",len(tr))\n",
        "print(\"number of testing  data: \",len(val))\n",
        "# dictionary with labels and ids of train data\n",
        "img_class_dict = {k:v for k, v in zip(labels.id, labels.label)}\n",
        "\n",
        "train_sampler = SubsetRandomSampler(list(tr.index))\n",
        "valid_sampler = SubsetRandomSampler(list(val.index))\n",
        "batch_size = 16 #256\n",
        "num_workers = 0\n",
        "#print(os.listdir('/content/drive/My Drive/Colab_data/train/'))\n",
        "dataset = CancerDataset(datafolder='/content/drive/My Drive/Colab_data/train/', datatype='train', transform=data_transforms, labels_dict=img_class_dict)\n",
        "test_set = CancerDataset(datafolder='/content/drive/My Drive/Colab_data/test/', datatype='test', transform=data_transforms_test)\n",
        "#print(len(dataset))\n",
        "\n",
        "# prepare data loaders (combine dataset and sampler)\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training data:  126\n",
            "number of testing  data:  15\n",
            "[1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9112b0d404cc38ead7693b7ee6c9cb5cf792d4b4",
        "id": "jMe5iFpNNjIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "def train(model, train_loader ,loss_fn, optimizer, num_epochs = 1):\n",
        "    total_loss =0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
        "        model.train()\n",
        "\n",
        "        print(train_loader)\n",
        "\n",
        "        for t, (x, y) in enumerate(train_loader):\n",
        "            x_var = Variable(x.type(gpu_dtype))\n",
        "            y_var = Variable(y.type(gpu_dtype).long())\n",
        "\n",
        "            scores = model(x_var)\n",
        "            loss = loss_fn(scores, y_var)\n",
        "            total_loss += loss.data\n",
        "            \n",
        "            if (t + 1) % print_every == 0:\n",
        "                avg_loss = total_loss/print_every\n",
        "                print('t = %d, avg_loss = %.4f' % (t + 1, avg_loss) )\n",
        "                avg_loss_list.append(avg_loss)\n",
        "                total_loss = 0\n",
        "                \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        acc = check_accuracy(fixed_model_gpu, valid_loader)\n",
        "        print('acc = %f' %(acc))\n",
        "            \n",
        "def check_accuracy(model, loader):\n",
        "    print('Checking accuracy on test set')   \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
        "    for x, y in loader:\n",
        "        x_var = Variable(x.type(gpu_dtype))\n",
        "\n",
        "        scores = model(x_var)\n",
        "        _, preds = scores.data.cpu().max(1)\n",
        "        num_correct += (preds == y).sum()\n",
        "        num_samples += preds.size(0)\n",
        "    acc = float(num_correct) / num_samples\n",
        "    acc_list.append(acc)\n",
        "    return acc\n",
        "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "90bae2c4a31beaf2945d2f54c9f092155bfed1a9",
        "id": "jnJ3u16XNjII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.size() # read in N, C, H, W\n",
        "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "cdcb7f6c9adffc8b1b2862773997e7f32726f5fc",
        "scrolled": false,
        "id": "L0QDuq7xNjIn",
        "colab_type": "code",
        "outputId": "c3c18c55-9556-4988-d9dd-c923d7e8352e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "print_every = 20\n",
        "gpu_dtype = torch.cuda.FloatTensor\n",
        "\n",
        "out_1 = 32\n",
        "out_2 = 64\n",
        "out_3 = 128\n",
        "out_4 = 256\n",
        "\n",
        "k_size_1 = 3\n",
        "padding_1 = 1\n",
        "\n",
        "\n",
        "num_epochs = 6\n",
        "\n",
        "# self.conv1=PolynomialKernelConv(1,10,5,cp=0.5,dp=2)\n",
        "\n",
        "\n",
        "fixed_model_base = nn.Sequential( \n",
        "    #nn.Kerv2d(3, out_1, kernel_size=k_size_1),                             \n",
        "    #KernelConv2d(3, out_1, kernel_size=k_size_1),\n",
        "                #nn.Conv2d(3, out_1, padding= padding_1, kernel_size=k_size_1, stride=1), # out_1-k_size_1+1 = 26\n",
        "                nn.Kerv2d(3, out_1, padding= padding_1, kernel_size=k_size_1, stride=1), # out_1-k_size_1+1 = 26\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_1),\n",
        "    #KernelConv2d\n",
        "    #KernelConv2d(out_1, out_1, kernel_size=k_size_1),\n",
        "                nn.Kerv2d(out_1 , out_1, padding= padding_1, kernel_size=k_size_1, stride=1), #26 - 4 + 1 = 23\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_1),\n",
        "      \n",
        "    #KernelConv2d(out_1, out_1, kernel_size=k_size_1),\n",
        "                nn.Conv2d(out_1 , out_1, padding= padding_1, kernel_size=k_size_1, stride=1), # 23 -3 = 20\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_1),\n",
        "    \n",
        "                nn.MaxPool2d(2, stride=2),\n",
        "    \n",
        "    #KernelConv2d(out_1, out_2, kernel_size=k_size_1),\n",
        "                nn.Conv2d(out_1 , out_2, padding= padding_1, kernel_size=k_size_1, stride=1), # 20 -3 = 17\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_2),\n",
        "   \n",
        "    #KernelConv2d(out_2, out_2, kernel_size=k_size_1),\n",
        "                nn.Conv2d(out_2 , out_2, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
        "                nn.ReLU(inplace=True), \n",
        "                nn.BatchNorm2d(out_2),\n",
        "    \n",
        "    #KernelConv2d(out_2, out_2, kernel_size=k_size_1),\n",
        "                nn.Conv2d(out_2 , out_2, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_2),\n",
        "    \n",
        "                nn.MaxPool2d(2, stride=2),\n",
        "    \n",
        "    #KernelConv2d(out_2, out_3, kernel_size=k_size_1),\n",
        "                nn.Conv2d(out_2 , out_3, padding= padding_1, kernel_size=k_size_1, stride=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_3),\n",
        "              \n",
        "    #KernelConv2d(out_3, out_3, kernel_size=k_size_1),\n",
        "                nn.Conv2d(out_3 , out_3, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_3),\n",
        "    #KernelConv2d(out_3, out_3, kernel_size=k_size_1),\n",
        "                nn.Conv2d(out_3 , out_3, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_3),\n",
        "    \n",
        "                nn.MaxPool2d(2, stride=2),\n",
        "    #KernelConv2d(out_3, out_4, kernel_size=k_size_1),\n",
        "                nn.Conv2d(out_3 , out_4, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_4),\n",
        "    #KernelConv2d(out_4, out_4, kernel_size=k_size_1),\n",
        "                nn.Conv2d(out_4 , out_4, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_4),\n",
        "    #KernelConv2d(out_4, out_4, kernel_size=k_size_1),\n",
        "                nn.Conv2d(out_4 , out_4, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(out_4),\n",
        "                        #nn.Conv2d(out_11 , out_12, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n",
        "                        #nn.ReLU(inplace=True),\n",
        "                        #nn.BatchNorm2d(out_12),\n",
        "    \n",
        "                nn.MaxPool2d(2, stride=2), #17/2 = 7\n",
        "                Flatten(),\n",
        "                \n",
        "                nn.Linear(9216,512 ), # affine layer\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(512,10), # affine layer\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(10,2), # affine layer\n",
        "            )\n",
        "fixed_model_gpu = fixed_model_base.type(gpu_dtype)\n",
        "print(fixed_model_gpu)\n",
        "loss_fn = nn.modules.loss.CrossEntropyLoss()\n",
        "optimizer = optim.RMSprop(fixed_model_gpu.parameters(), lr = 1e-3)\n",
        "\n",
        "train(fixed_model_gpu, train_loader ,loss_fn, optimizer, num_epochs=num_epochs)\n",
        "check_accuracy(fixed_model_gpu, valid_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-03869f8b605c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# affine layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             )\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mfixed_model_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_model_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_model_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, dst_type)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "6b2f26fc0e0f59be1eba5774d5d68c1b6f581ef6",
        "id": "eyy46TWjNjIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(avg_loss_list,acc_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "6849f23812e7deb06c2500395f80015115bfce2f",
        "id": "E9li0OXuNjI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([print_every*batch_size*(i+1)/len(tr) for i in range((len(avg_loss_list)))],avg_loss_list)\n",
        "plt.plot([i+1 for i in range((len(acc_list)))],acc_list)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "6088c735b19731d3c5cfdd28811ceca346cabc66",
        "id": "bXfgfMsoNjI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fixed_model_gpu.eval()\n",
        "preds = []\n",
        "for batch_i, (data, target) in enumerate(test_loader):\n",
        "    data, target = data.cuda(), target.cuda()\n",
        "    output = fixed_model_gpu(data)\n",
        "\n",
        "    pr = output[:,1].detach().cpu().numpy()\n",
        "    for i in pr:\n",
        "        preds.append(i)\n",
        "        \n",
        "test_preds = pd.DataFrame({'imgs': test_set.image_files_list, 'preds': preds})\n",
        "\n",
        "test_preds['imgs'] = test_preds['imgs'].apply(lambda x: x.split('.')[0])\n",
        "\n",
        "data_to_submit = pd.read_csv('/content/drive/My Drive/Colab_data//sample_submission.csv')\n",
        "data_to_submit = pd.merge(data_to_submit, test_preds, left_on='id', right_on='imgs')\n",
        "data_to_submit = data_to_submit[['id', 'preds']]\n",
        "data_to_submit.columns = ['id', 'label']\n",
        "data_to_submit.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4011f2fe1ba4c87cd64cc792ee37fd7977b65432",
        "id": "QRpWoC9NNjI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_to_submit.to_csv('csv_to_submit.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "7a4baf77ec397302c7c35af473595f74b6cf240c",
        "id": "uMcwgrA_NjJE",
        "colab_type": "text"
      },
      "source": [
        "#citation\n",
        "* data parsing and code for submittion are taken from: https://www.kaggle.com/artgor/simple-eda-and-model-in-pytorch"
      ]
    }
  ]
}